{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ å›¾æ–‡æ£€ç´¢ç³»ç»Ÿ - CLIP + FAISS\n",
    "\n",
    "æœ¬Notebookå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„å›¾æ–‡æ£€ç´¢ç³»ç»Ÿï¼š\n",
    "- ä½¿ç”¨ **CLIP** æ¨¡å‹å¯¹å›¾ç‰‡å’Œæ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "- ä½¿ç”¨ **FAISS** å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢\n",
    "- æ”¯æŒ **æ–‡æœ¬æ£€ç´¢å›¾ç‰‡** å’Œ **å›¾ç‰‡æ£€ç´¢æ–‡æœ¬**\n",
    "\n",
    "## åŠŸèƒ½ç‰¹ç‚¹\n",
    "1. æ–‡æœ¬æ£€ç´¢å›¾ç‰‡ï¼šè¾“å…¥æè¿°ï¼Œè¿”å›æœ€ç›¸å…³çš„å›¾ç‰‡\n",
    "2. å›¾ç‰‡æ£€ç´¢æ–‡æœ¬ï¼šè¾“å…¥å›¾ç‰‡ï¼Œè¿”å›æœ€ç›¸å…³çš„æè¿°\n",
    "3. ä»¥å›¾æœå›¾ï¼šè¾“å…¥å›¾ç‰‡ï¼Œè¿”å›ç›¸ä¼¼çš„å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q ftfy regex tqdm\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥åº“\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import clip\n",
    "import requests\n",
    "import zipfile\n",
    "from pycocotools.coco import COCO\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å®šä¹‰å›¾æ–‡æ£€ç´¢ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextRetrieval:\n",
    "    def __init__(self, device=None, clip_model=\"ViT-B/32\", index_type=\"flat\"):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "        \n",
    "        self.clip_model_name = clip_model\n",
    "        self.index_type = index_type\n",
    "        \n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.image_index = None\n",
    "        self.text_index = None\n",
    "        self.image_embeddings = None\n",
    "        self.text_embeddings = None\n",
    "        self.image_paths = []\n",
    "        self.texts = []\n",
    "        \n",
    "        self._load_clip_model()\n",
    "    \n",
    "    def _load_clip_model(self):\n",
    "        print(f\"ğŸ“¥ åŠ è½½CLIPæ¨¡å‹: {self.clip_model_name}\")\n",
    "        self.model, self.preprocess = clip.load(self.clip_model_name, device=self.device)\n",
    "        self.model.eval()\n",
    "        print(\"âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    def encode_images(self, image_paths, batch_size=32):\n",
    "        print(f\"ğŸ–¼ï¸ ç¼–ç  {len(image_paths)} å¼ å›¾ç‰‡...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(path).convert('RGB')\n",
    "                    image = self.preprocess(image)\n",
    "                    batch_images.append(image)\n",
    "                except Exception as e:\n",
    "                    batch_images.append(torch.zeros(3, 224, 224))\n",
    "            \n",
    "            batch_images = torch.stack(batch_images).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model.encode_image(batch_images)\n",
    "                batch_embeddings = F.normalize(batch_embeddings, dim=-1)\n",
    "            \n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        \n",
    "        self.image_embeddings = np.vstack(embeddings).astype('float32')\n",
    "        self.image_paths = image_paths\n",
    "        print(f\"âœ… å›¾ç‰‡ç¼–ç å®Œæˆï¼Œå½¢çŠ¶: {self.image_embeddings.shape}\")\n",
    "        return self.image_embeddings\n",
    "    \n",
    "    def encode_texts(self, texts, batch_size=32):\n",
    "        print(f\"ğŸ“ ç¼–ç  {len(texts)} æ¡æ–‡æœ¬...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                text_tokens = clip.tokenize(batch_texts, truncate=True).to(self.device)\n",
    "                batch_embeddings = self.model.encode_text(text_tokens)\n",
    "                batch_embeddings = F.normalize(batch_embeddings, dim=-1)\n",
    "            \n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        \n",
    "        self.text_embeddings = np.vstack(embeddings).astype('float32')\n",
    "        self.texts = texts\n",
    "        print(f\"âœ… æ–‡æœ¬ç¼–ç å®Œæˆï¼Œå½¢çŠ¶: {self.text_embeddings.shape}\")\n",
    "        return self.text_embeddings\n",
    "    \n",
    "    def build_index(self, embeddings, nlist=100, nprobe=10):\n",
    "        d = embeddings.shape[1]\n",
    "        \n",
    "        if self.index_type == \"flat\":\n",
    "            index = faiss.IndexFlatIP(d)\n",
    "        elif self.index_type == \"ivf\":\n",
    "            quantizer = faiss.IndexFlatIP(d)\n",
    "            index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "            index.train(embeddings)\n",
    "            index.nprobe = nprobe\n",
    "        elif self.index_type == \"hnsw\":\n",
    "            M = 32\n",
    "            index = faiss.IndexHNSWFlat(d, M)\n",
    "            index.hnsw.efSearch = 64\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {self.index_type}\")\n",
    "        \n",
    "        index.add(embeddings)\n",
    "        print(f\"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡\")\n",
    "        return index\n",
    "    \n",
    "    def build_image_index(self, nlist=100):\n",
    "        if self.image_embeddings is None:\n",
    "            raise ValueError(\"è¯·å…ˆç¼–ç å›¾ç‰‡\")\n",
    "        self.image_index = self.build_index(self.image_embeddings, nlist)\n",
    "        return self.image_index\n",
    "    \n",
    "    def build_text_index(self, nlist=100):\n",
    "        if self.text_embeddings is None:\n",
    "            raise ValueError(\"è¯·å…ˆç¼–ç æ–‡æœ¬\")\n",
    "        self.text_index = self.build_index(self.text_embeddings, nlist)\n",
    "        return self.text_index\n",
    "    \n",
    "    def search_text_to_image(self, query, k=5):\n",
    "        with torch.no_grad():\n",
    "            text_token = clip.tokenize([query], truncate=True).to(self.device)\n",
    "            query_embedding = self.model.encode_text(text_token)\n",
    "            query_embedding = F.normalize(query_embedding, dim=-1)\n",
    "            query_embedding = query_embedding.cpu().numpy().astype('float32')\n",
    "        \n",
    "        D, I = self.image_index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx < len(self.image_paths):\n",
    "                results.append((self.image_paths[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_image_to_text(self, image_path, k=5):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode_image(image)\n",
    "            query_embedding = F.normalize(query_embedding, dim=-1)\n",
    "            query_embedding = query_embedding.cpu().numpy().astype('float32')\n",
    "        \n",
    "        D, I = self.text_index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx < len(self.texts):\n",
    "                results.append((self.texts[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_image_to_image(self, image_path, k=5):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode_image(image)\n",
    "            query_embedding = F.normalize(query_embedding, dim=-1)\n",
    "            query_embedding = query_embedding.cpu().numpy().astype('float32')\n",
    "        \n",
    "        D, I = self.image_index.search(query_embedding, k + 1)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx < len(self.image_paths) and self.image_paths[idx] != image_path:\n",
    "                results.append((self.image_paths[idx], float(score)))\n",
    "        \n",
    "        return results[:k]\n",
    "    \n",
    "    def visualize_text_to_image(self, query, results):\n",
    "        n = len(results)\n",
    "        fig, axes = plt.subplots(1, n + 1, figsize=(4 * (n + 1), 4))\n",
    "        \n",
    "        axes[0].text(0.5, 0.5, query, ha='center', va='center', fontsize=12, wrap=True,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "        axes[0].set_title(\"ğŸ” æŸ¥è¯¢æ–‡æœ¬\", fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        for i, (img_path, score) in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                axes[i + 1].imshow(img)\n",
    "                axes[i + 1].set_title(f\"ç›¸ä¼¼åº¦: {score:.4f}\", fontsize=10)\n",
    "            except:\n",
    "                axes[i + 1].text(0.5, 0.5, \"åŠ è½½å¤±è´¥\", ha='center', va='center')\n",
    "            axes[i + 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_image_to_text(self, image_path, results):\n",
    "        n = len(results)\n",
    "        fig, axes = plt.subplots(1, n + 1, figsize=(4 * (n + 1), 4))\n",
    "        \n",
    "        query_img = Image.open(image_path).convert('RGB')\n",
    "        axes[0].imshow(query_img)\n",
    "        axes[0].set_title(\"ğŸ” æŸ¥è¯¢å›¾ç‰‡\", fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        for i, (text, score) in enumerate(results):\n",
    "            axes[i + 1].text(0.5, 0.5, text, ha='center', va='center', fontsize=9, wrap=True,\n",
    "                           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "            axes[i + 1].set_title(f\"ç›¸ä¼¼åº¦: {score:.4f}\", fontsize=10)\n",
    "            axes[i + 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¸‹è½½COCOæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_coco_dataset(sample_size=1000, save_dir=\"./coco_data\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ“¥ ä¸‹è½½COCOæ ‡æ³¨æ–‡ä»¶...\")\n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    ann_zip = os.path.join(save_dir, \"annotations.zip\")\n",
    "    \n",
    "    if not os.path.exists(ann_zip):\n",
    "        response = requests.get(ann_url, stream=True)\n",
    "        with open(ann_zip, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        with zipfile.ZipFile(ann_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "    \n",
    "    ann_file = os.path.join(save_dir, \"annotations\", \"captions_val2017.json\")\n",
    "    coco = COCO(ann_file)\n",
    "    \n",
    "    img_dir = os.path.join(save_dir, \"images\")\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    \n",
    "    img_ids = list(coco.imgs.keys())[:sample_size]\n",
    "    \n",
    "    image_paths = []\n",
    "    texts = []\n",
    "    \n",
    "    print(f\"ğŸ“¥ ä¸‹è½½ {len(img_ids)} å¼ å›¾ç‰‡...\")\n",
    "    for img_id in tqdm(img_ids):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info['coco_url']\n",
    "        img_path = os.path.join(img_dir, img_info['file_name'])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            try:\n",
    "                response = requests.get(img_url, stream=True, timeout=10)\n",
    "                with open(img_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        image_paths.append(img_path)\n",
    "        \n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        if anns:\n",
    "            texts.append(anns[0]['caption'])\n",
    "    \n",
    "    print(f\"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(image_paths)} å¼ å›¾ç‰‡, {len(texts)} æ¡æ–‡æœ¬\")\n",
    "    return image_paths, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½COCOæ•°æ®é›†ï¼ˆå¯ä»¥è°ƒæ•´sample_sizeï¼‰\n",
    "SAMPLE_SIZE = 500  # å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "image_paths, texts = download_coco_dataset(sample_size=SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ„å»ºç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ\n",
    "retrieval = ImageTextRetrieval(device=\"cuda\", index_type=\"flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¼–ç å›¾ç‰‡å’Œæ–‡æœ¬\n",
    "retrieval.encode_images(image_paths, batch_size=64)\n",
    "retrieval.encode_texts(texts, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºç´¢å¼•\n",
    "retrieval.build_image_index()\n",
    "retrieval.build_text_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ–‡æœ¬æ£€ç´¢å›¾ç‰‡æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æŸ¥è¯¢\n",
    "test_queries = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"people walking on the street\",\n",
    "    \"a car on the road\",\n",
    "    \"a beautiful sunset over the ocean\",\n",
    "    \"children playing with toys\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” æŸ¥è¯¢: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = retrieval.search_text_to_image(query, k=5)\n",
    "    \n",
    "    for i, (path, score) in enumerate(results):\n",
    "        print(f\"  {i+1}. {os.path.basename(path)} (ç›¸ä¼¼åº¦: {score:.4f})\")\n",
    "    \n",
    "    retrieval.visualize_text_to_image(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å›¾ç‰‡æ£€ç´¢æ–‡æœ¬æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºé€‰æ‹©å‡ å¼ å›¾ç‰‡è¿›è¡Œæµ‹è¯•\n",
    "import random\n",
    "test_images = random.sample(image_paths, 3)\n",
    "\n",
    "for img_path in test_images:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ–¼ï¸ æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(img_path)}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = retrieval.search_image_to_text(img_path, k=3)\n",
    "    \n",
    "    for i, (text, score) in enumerate(results):\n",
    "        print(f\"  {i+1}. {text} (ç›¸ä¼¼åº¦: {score:.4f})\")\n",
    "    \n",
    "    retrieval.visualize_image_to_text(img_path, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ä»¥å›¾æœå›¾æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥å›¾æœå›¾\n",
    "test_img = random.choice(image_paths)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ–¼ï¸ æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(test_img)}\")\n",
    "print('='*60)\n",
    "\n",
    "results = retrieval.search_image_to_image(test_img, k=5)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# æ˜¾ç¤ºæŸ¥è¯¢å›¾ç‰‡\n",
    "query_img = Image.open(test_img).convert('RGB')\n",
    "axes[0, 0].imshow(query_img)\n",
    "axes[0, 0].set_title(\"æŸ¥è¯¢å›¾ç‰‡\", fontsize=12, color='red')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "for i, (path, score) in enumerate(results[:5]):\n",
    "    row, col = (i + 1) // 3, (i + 1) % 3\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].set_title(f\"ç›¸ä¼¼åº¦: {score:.4f}\", fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# éšè—ç©ºç™½å­å›¾\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è‡ªå®šä¹‰æŸ¥è¯¢æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨è¿™é‡Œè¾“å…¥ä½ è‡ªå·±çš„æŸ¥è¯¢\n",
    "my_query = \"a person riding a bicycle\"  # ä¿®æ”¹è¿™é‡Œ\n",
    "\n",
    "print(f\"ğŸ” æŸ¥è¯¢: {my_query}\")\n",
    "results = retrieval.search_text_to_image(my_query, k=5)\n",
    "\n",
    "for i, (path, score) in enumerate(results):\n",
    "    print(f\"  {i+1}. {os.path.basename(path)} (ç›¸ä¼¼åº¦: {score:.4f})\")\n",
    "\n",
    "retrieval.visualize_text_to_image(my_query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# æµ‹è¯•æ£€ç´¢é€Ÿåº¦\n",
    "query = \"a dog running in the park\"\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    results = retrieval.search_text_to_image(query, k=5)\n",
    "end_time = time.time()\n",
    "\n",
    "avg_time = (end_time - start_time) / 100 * 1000\n",
    "print(f\"ğŸ“Š å¹³å‡æ£€ç´¢æ—¶é—´: {avg_time:.2f} ms\")\n",
    "print(f\"ğŸ“Š ç´¢å¼•å¤§å°: {retrieval.image_index.ntotal} ä¸ªå‘é‡\")\n",
    "print(f\"ğŸ“Š å‘é‡ç»´åº¦: {retrieval.image_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€»ç»“\n",
    "\n",
    "### æœ¬ç³»ç»Ÿå®ç°äº†ï¼š\n",
    "1. **æ–‡æœ¬æ£€ç´¢å›¾ç‰‡** - è¾“å…¥è‡ªç„¶è¯­è¨€æè¿°ï¼Œè¿”å›ç›¸å…³å›¾ç‰‡\n",
    "2. **å›¾ç‰‡æ£€ç´¢æ–‡æœ¬** - è¾“å…¥å›¾ç‰‡ï¼Œè¿”å›ç›¸å…³æè¿°\n",
    "3. **ä»¥å›¾æœå›¾** - è¾“å…¥å›¾ç‰‡ï¼Œè¿”å›ç›¸ä¼¼å›¾ç‰‡\n",
    "\n",
    "### æŠ€æœ¯æ ˆï¼š\n",
    "- **CLIP**: OpenAIçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†å›¾ç‰‡å’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€å‘é‡ç©ºé—´\n",
    "- **FAISS**: Facebookçš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“ï¼Œæ”¯æŒå¤§è§„æ¨¡ç›¸ä¼¼åº¦æœç´¢\n",
    "\n",
    "### æ‰©å±•æ–¹å‘ï¼š\n",
    "1. ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†\n",
    "2. å°è¯•ä¸åŒçš„CLIPæ¨¡å‹ï¼ˆå¦‚ViT-L/14ï¼‰\n",
    "3. ä½¿ç”¨FAISSçš„IVFæˆ–HNSWç´¢å¼•åŠ é€Ÿå¤§è§„æ¨¡æ£€ç´¢\n",
    "4. æ·»åŠ é‡æ’åºæ¨¡å—æå‡ç²¾åº¦"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
