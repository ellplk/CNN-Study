{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ BLIP: Bootstrap Language-Image Pre-training\n",
    "\n",
    "BLIPæ˜¯Salesforceå¼€å‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒï¼š\n",
    "- **å›¾åƒæè¿°ç”Ÿæˆ** (Image Captioning)\n",
    "- **è§†è§‰é—®ç­”** (Visual Question Answering)\n",
    "- **å›¾æ–‡æ£€ç´¢** (Image-Text Retrieval)\n",
    "\n",
    "## BLIP vs CLIP å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | CLIP | BLIP |\n",
    "|------|------|------|\n",
    "| å›¾åƒæè¿°ç”Ÿæˆ | âŒ | âœ… |\n",
    "| è§†è§‰é—®ç­” | âŒ | âœ… |\n",
    "| å›¾æ–‡æ£€ç´¢ | âœ… | âœ… |\n",
    "| å¤šç›®æ ‡ç†è§£ | å¼± | å¼º |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q transformers accelerate timm\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q ftfy regex tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    BlipForQuestionAnswering,\n",
    "    BlipForImageTextRetrieval\n",
    ")\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½BLIPæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPSystem:\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        print(\"ğŸ“¥ åŠ è½½BLIPæ¨¡å‹...\")\n",
    "        \n",
    "        # å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹\n",
    "        self.caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.caption_model = BlipForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip-image-captioning-base\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # è§†è§‰é—®ç­”æ¨¡å‹\n",
    "        self.vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "        self.vqa_model = BlipForQuestionAnswering.from_pretrained(\n",
    "            \"Salesforce/blip-vqa-base\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # å›¾æ–‡æ£€ç´¢æ¨¡å‹\n",
    "        self.retrieval_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "        self.retrieval_model = BlipForImageTextRetrieval.from_pretrained(\n",
    "            \"Salesforce/blip-itm-base-coco\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(\"âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    def generate_caption(self, image, max_length=50):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå›¾åƒæè¿°\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Imageæˆ–å›¾ç‰‡è·¯å¾„\n",
    "            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
    "            \n",
    "        Returns:\n",
    "            ç”Ÿæˆçš„æè¿°æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        inputs = self.caption_processor(image, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.caption_model.generate(\n",
    "                **inputs, \n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                temperature=1.0,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "            )\n",
    "        \n",
    "        caption = self.caption_processor.decode(output[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def answer_question(self, image, question, max_length=30):\n",
    "        \"\"\"\n",
    "        è§†è§‰é—®ç­”\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Imageæˆ–å›¾ç‰‡è·¯å¾„\n",
    "            question: é—®é¢˜æ–‡æœ¬\n",
    "            max_length: æœ€å¤§ç­”æ¡ˆé•¿åº¦\n",
    "            \n",
    "        Returns:\n",
    "            ç­”æ¡ˆæ–‡æœ¬\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        inputs = self.vqa_processor(image, question, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.vqa_model.generate(\n",
    "                **inputs, \n",
    "                max_length=max_length\n",
    "            )\n",
    "        \n",
    "        answer = self.vqa_processor.decode(output[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "    \n",
    "    def compute_similarity(self, image, text):\n",
    "        \"\"\"\n",
    "        è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Imageæˆ–å›¾ç‰‡è·¯å¾„\n",
    "            text: æ–‡æœ¬æè¿°\n",
    "            \n",
    "        Returns:\n",
    "            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        inputs = self.retrieval_processor(image, text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.retrieval_model(**inputs, use_itm_head=True)\n",
    "            similarity = torch.softmax(output[0], dim=1)[0, 1].item()\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        æå–å›¾åƒç‰¹å¾å‘é‡\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Imageæˆ–å›¾ç‰‡è·¯å¾„\n",
    "            \n",
    "        Returns:\n",
    "            ç‰¹å¾å‘é‡ (numpy array)\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        inputs = self.retrieval_processor(image, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.retrieval_model.vision_model(\n",
    "                pixel_values=inputs['pixel_values']\n",
    "            )\n",
    "            image_embeds = vision_outputs[1]\n",
    "            image_embeds = self.retrieval_model.vision_proj(image_embeds)\n",
    "            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_embeds.cpu().numpy().astype('float32')\n",
    "\n",
    "# åˆå§‹åŒ–ç³»ç»Ÿ\n",
    "blip = BLIPSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å›¾åƒæè¿°ç”Ÿæˆæ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"ä»URLåŠ è½½å›¾ç‰‡\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "def visualize_caption(image, caption):\n",
    "    \"\"\"å¯è§†åŒ–å›¾åƒæè¿°ç»“æœ\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(caption, fontsize=14, wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å›¾ç‰‡URL\n",
    "test_images = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",  # çŒ«\n",
    "    \"http://images.cocodataset.org/val2017/000000397133.jpg\",  # äººåœ¨å†²æµª\n",
    "    \"http://images.cocodataset.org/val2017/000000252219.jpg\",  # å¨æˆ¿åœºæ™¯\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“¸ å›¾åƒæè¿°ç”Ÿæˆæ¼”ç¤º\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, url in enumerate(test_images):\n",
    "    print(f\"\\n--- å›¾ç‰‡ {i+1} ---\")\n",
    "    \n",
    "    # åŠ è½½å›¾ç‰‡\n",
    "    image = load_image_from_url(url)\n",
    "    \n",
    "    # ç”Ÿæˆæè¿°\n",
    "    caption = blip.generate_caption(image)\n",
    "    \n",
    "    print(f\"ç”Ÿæˆçš„æè¿°: {caption}\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    visualize_caption(image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è§†è§‰é—®ç­”æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vqa(image, qa_pairs):\n",
    "    \"\"\"å¯è§†åŒ–VQAç»“æœ\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # æ˜¾ç¤ºå›¾ç‰‡\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"æŸ¥è¯¢å›¾ç‰‡\", fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # æ˜¾ç¤ºé—®ç­”\n",
    "    qa_text = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in qa_pairs])\n",
    "    axes[1].text(0.1, 0.5, qa_text, fontsize=12, va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "    axes[1].set_title(\"è§†è§‰é—®ç­”ç»“æœ\", fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQAæµ‹è¯•\n",
    "print(\"=\"*60)\n",
    "print(\"â“ è§†è§‰é—®ç­”æ¼”ç¤º\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨ç¬¬ä¸€å¼ æµ‹è¯•å›¾ç‰‡\n",
    "image = load_image_from_url(test_images[0])\n",
    "\n",
    "questions = [\n",
    "    \"What animal is in the image?\",\n",
    "    \"How many animals are there?\",\n",
    "    \"What color are the animals?\",\n",
    "    \"Where are the animals?\",\n",
    "    \"What are the animals doing?\"\n",
    "]\n",
    "\n",
    "qa_pairs = []\n",
    "for question in questions:\n",
    "    answer = blip.answer_question(image, question)\n",
    "    qa_pairs.append((question, answer))\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")\n",
    "\n",
    "visualize_vqa(image, qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å¤šç›®æ ‡åœºæ™¯ç†è§£æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ å¤šç›®æ ‡åœºæ™¯ç†è§£\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å¤æ‚åœºæ™¯å›¾ç‰‡\n",
    "complex_scene_url = \"http://images.cocodataset.org/val2017/000000439265.jpg\"\n",
    "image = load_image_from_url(complex_scene_url)\n",
    "\n",
    "# ç”Ÿæˆæ•´ä½“æè¿°\n",
    "caption = blip.generate_caption(image)\n",
    "print(f\"\\næ•´ä½“æè¿°: {caption}\")\n",
    "\n",
    "# é’ˆå¯¹æ€§æé—®\n",
    "questions = [\n",
    "    \"What objects are in the image?\",\n",
    "    \"How many people are there?\",\n",
    "    \"What is the background?\",\n",
    "    \"What colors can you see?\",\n",
    "    \"What is happening in the scene?\"\n",
    "]\n",
    "\n",
    "print(\"\\né’ˆå¯¹æ€§é—®ç­”:\")\n",
    "qa_pairs = []\n",
    "for question in questions:\n",
    "    answer = blip.answer_question(image, question)\n",
    "    qa_pairs.append((question, answer))\n",
    "    print(f\"  Q: {question}\")\n",
    "    print(f\"  A: {answer}\\n\")\n",
    "\n",
    "visualize_vqa(image, qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å›¾æ–‡æ£€ç´¢åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPRetrieval:\n",
    "    def __init__(self, blip_system):\n",
    "        self.blip = blip_system\n",
    "        self.image_index = None\n",
    "        self.image_paths = []\n",
    "        self.image_embeddings = []\n",
    "    \n",
    "    def add_images(self, image_paths, batch_size=32):\n",
    "        \"\"\"æ·»åŠ å›¾ç‰‡åˆ°ç´¢å¼•\"\"\"\n",
    "        print(f\"ğŸ“¥ å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡...\")\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_embeds = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    embed = self.blip.extract_features(path)\n",
    "                    batch_embeds.append(embed)\n",
    "                    self.image_paths.append(path)\n",
    "                except Exception as e:\n",
    "                    print(f\"å¤„ç†å¤±è´¥ {path}: {e}\")\n",
    "            \n",
    "            if batch_embeds:\n",
    "                embeddings.extend(batch_embeds)\n",
    "        \n",
    "        self.image_embeddings = np.vstack(embeddings).astype('float32')\n",
    "        print(f\"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå½¢çŠ¶: {self.image_embeddings.shape}\")\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"æ„å»ºFAISSç´¢å¼•\"\"\"\n",
    "        d = self.image_embeddings.shape[1]\n",
    "        self.image_index = faiss.IndexFlatIP(d)\n",
    "        self.image_index.add(self.image_embeddings)\n",
    "        print(f\"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {self.image_index.ntotal} ä¸ªå‘é‡\")\n",
    "    \n",
    "    def search(self, query_text, k=5):\n",
    "        \"\"\"æ–‡æœ¬æ£€ç´¢å›¾ç‰‡\"\"\"\n",
    "        # ç¼–ç æ–‡æœ¬\n",
    "        inputs = self.blip.retrieval_processor(\n",
    "            text=query_text, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.blip.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_outputs = self.blip.retrieval_model.text_encoder(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask']\n",
    "            )\n",
    "            text_embeds = text_outputs[0][:, 0, :]\n",
    "            text_embeds = self.blip.retrieval_model.text_proj(text_embeds)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        query_embed = text_embeds.cpu().numpy().astype('float32')\n",
    "        \n",
    "        # æœç´¢\n",
    "        D, I = self.image_index.search(query_embed, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx < len(self.image_paths):\n",
    "                results.append((self.image_paths[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ\n",
    "retrieval = BLIPRetrieval(blip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½COCOæ ·æœ¬å›¾ç‰‡\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"./sample_images\", exist_ok=True)\n",
    "\n",
    "sample_urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000397133.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000252219.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000439265.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000001268.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000002153.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000002473.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000002529.jpg\",\n",
    "]\n",
    "\n",
    "image_paths = []\n",
    "for i, url in enumerate(sample_urls):\n",
    "    path = f\"./sample_images/image_{i}.jpg\"\n",
    "    if not os.path.exists(path):\n",
    "        response = requests.get(url)\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    image_paths.append(path)\n",
    "\n",
    "print(f\"âœ… ä¸‹è½½äº† {len(image_paths)} å¼ å›¾ç‰‡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºç´¢å¼•\n",
    "retrieval.add_images(image_paths)\n",
    "retrieval.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–‡æœ¬æ£€ç´¢å›¾ç‰‡æµ‹è¯•\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” æ–‡æœ¬æ£€ç´¢å›¾ç‰‡æ¼”ç¤º\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"a cat sleeping on a bed\",\n",
    "    \"people surfing in the ocean\",\n",
    "    \"a kitchen with appliances\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\næŸ¥è¯¢: {query}\")\n",
    "    results = retrieval.search(query, k=3)\n",
    "    \n",
    "    for i, (path, score) in enumerate(results):\n",
    "        print(f\"  {i+1}. {os.path.basename(path)} (ç›¸ä¼¼åº¦: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. BLIP vs CLIP å¯¹æ¯”å®éªŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š BLIP vs CLIP åŠŸèƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æµ‹è¯•å›¾ç‰‡\n",
    "test_image = load_image_from_url(test_images[0])\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ å›¾åƒæè¿°ç”Ÿæˆ:\")\n",
    "print(f\"   BLIP: {blip.generate_caption(test_image)}\")\n",
    "print(f\"   CLIP: âŒ ä¸æ”¯æŒ\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ è§†è§‰é—®ç­”:\")\n",
    "question = \"What animal is this?\"\n",
    "print(f\"   é—®é¢˜: {question}\")\n",
    "print(f\"   BLIP: {blip.answer_question(test_image, question)}\")\n",
    "print(f\"   CLIP: âŒ ä¸æ”¯æŒ\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ å›¾æ–‡ç›¸ä¼¼åº¦:\")\n",
    "text = \"two cats sleeping on a pink blanket\"\n",
    "print(f\"   æ–‡æœ¬: {text}\")\n",
    "print(f\"   BLIPç›¸ä¼¼åº¦: {blip.compute_similarity(test_image, text):.4f}\")\n",
    "print(f\"   CLIP: âœ… æ”¯æŒï¼ˆéœ€å•ç‹¬å®ç°ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è‡ªå®šä¹‰å›¾ç‰‡æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šä¼ è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œæµ‹è¯•\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ğŸ“¤ è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“¸ å›¾ç‰‡: {filename}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ç”Ÿæˆæè¿°\n",
    "    caption = blip.generate_caption(image)\n",
    "    print(f\"\\nğŸ“ è‡ªåŠ¨æè¿°: {caption}\")\n",
    "    \n",
    "    # VQA\n",
    "    questions = [\n",
    "        \"What is in this image?\",\n",
    "        \"What colors are dominant?\",\n",
    "        \"What is the main subject?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nâ“ è§†è§‰é—®ç­”:\")\n",
    "    for q in questions:\n",
    "        a = blip.answer_question(image, q)\n",
    "        print(f\"   Q: {q}\")\n",
    "        print(f\"   A: {a}\\n\")\n",
    "    \n",
    "    visualize_caption(image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æ€»ç»“\n",
    "\n",
    "### BLIP æ ¸å¿ƒä¼˜åŠ¿\n",
    "\n",
    "1. **å›¾åƒæè¿°ç”Ÿæˆ** - è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°\n",
    "2. **è§†è§‰é—®ç­”** - ç†è§£å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜\n",
    "3. **å¤šç›®æ ‡ç†è§£** - é€šè¿‡VQAå¯ä»¥è¯¢é—®å›¾ç‰‡ä¸­çš„å…·ä½“ç‰©ä½“\n",
    "4. **å›¾æ–‡æ£€ç´¢** - æ”¯æŒè·¨æ¨¡æ€æ£€ç´¢\n",
    "\n",
    "### è§£å†³å¤šç›®æ ‡é—®é¢˜çš„æ–¹æ³•\n",
    "\n",
    "| æ–¹æ³• | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **VQA** | é€šè¿‡æé—®è·å–å…·ä½“ç‰©ä½“ä¿¡æ¯ |\n",
    "| **æè¿°ç”Ÿæˆ** | è‡ªåŠ¨åˆ—ä¸¾å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ |\n",
    "| **ç›¸ä¼¼åº¦åŒ¹é…** | æ£€ç´¢åŒ…å«ç‰¹å®šç‰©ä½“çš„å›¾ç‰‡ |\n",
    "\n",
    "### é€‚ç”¨åœºæ™¯\n",
    "\n",
    "- å›¾åƒè‡ªåŠ¨æ ‡æ³¨\n",
    "- æ™ºèƒ½ç›¸å†Œç®¡ç†\n",
    "- è§†è§‰å†…å®¹ç†è§£\n",
    "- è¾…åŠ©è§†è§‰é—®ç­”ç³»ç»Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
