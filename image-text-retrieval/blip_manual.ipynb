{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95047ab0",
   "metadata": {},
   "source": [
    "# 从零实现BLIP模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065f942",
   "metadata": {},
   "source": [
    "# 1.安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0858a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd97276",
   "metadata": {},
   "source": [
    "# 2. 实现Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    将图像分割成patch并进行嵌入\n",
    "    \n",
    "    输入: (B, 3, 224, 224)\n",
    "    输出: (B, 196, 768)  # 14x14 = 196 patches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # 14x14 = 196\n",
    "        \n",
    "        # 使用卷积将patch转换为向量\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, 224, 224)\n",
    "        x = self.proj(x)  # (B, 768, 14, 14)\n",
    "        x = x.flatten(2)  # (B, 768, 196)\n",
    "        x = x.transpose(1, 2)  # (B, 196, 768)\n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "patch_embed = PatchEmbedding()\n",
    "test_img = torch.randn(2, 3, 224, 224)\n",
    "output = patch_embed(test_img)\n",
    "print(f\"Patch Embedding输出形状: {output.shape}\")  # (2, 196, 768)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f332a",
   "metadata": {},
   "source": [
    "# 3. 实现Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力机制\n",
    "\n",
    "    支持三种模式：\n",
    "    1.Self-Attention: Q = K = V\n",
    "    2.Cross-Attention: Q来自一个序列，K和V来自另一个序列\n",
    "    3.Masked Self-Attention: 用于解码器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Q, K, V投影\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key=None, value=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (B, N, C) 查询序列\n",
    "            key: (B, M, C) 键序列 (None则使用query)\n",
    "            value: (B, M, C) 值序列 (None则使用query)\n",
    "            attention_mask: (B, 1, N, M) 注意力掩码\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "        \n",
    "        B, N, C = query.shape\n",
    "        \n",
    "        # 线形投影并重塑为多头形式\n",
    "        q = self.q_proj(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).reshape(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).reshape(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # 应用注意力掩码 (如果有)\n",
    "        if attention_mask is not None:\n",
    "            attn = attn + attention_mask\n",
    "        \n",
    "        # softmax归一化\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # 加权求和\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "attn = MultiHeadAttention()\n",
    "test_q = torch.randn(2, 196, 768)\n",
    "output = attn(test_q)\n",
    "print(f\"Multi-Head Attention输出形状: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0dfba",
   "metadata": {},
   "source": [
    "# 4. 实现Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f333dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"前馈神经网络\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Pre-Norm架构\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), attention_mask=attention_mask))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23dec7",
   "metadata": {},
   "source": [
    "# 5. 实现Vision Encoder (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    视觉编码器（ViT架构）\n",
    "    输入: (B, 3, 224, 224)\n",
    "    输出: \n",
    "        - cls_output: (B, 768) 全局特征\n",
    "        - patch_output: (B, 196, 768) 局部特征\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio = 4.0,dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # cls token 和位置编码\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        #  transformer encoder\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 初始化权重\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, 196, 768)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, 768)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, 197, 768)\n",
    "        x = self.pos_drop(x + self.pos_embed)  # (B, 197, 768)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # (B, 197, 768)\n",
    "        x = self.norm(x)  # (B, 197, 768)\n",
    "        cls_output = x[:, 0]  # (B, 768)\n",
    "        patch_output = x[:, 1:]  # (B, 196, 768)\n",
    "        return cls_output, patch_output\n",
    "\n",
    "# 测试\n",
    "print(\"测试Vision Encoder...\")\n",
    "vision_encoder = VisionEncoder(depth=4)  # 使用较少层数快速测试\n",
    "test_img = torch.randn(2, 3, 224, 224)\n",
    "cls_out, patch_out = vision_encoder(test_img)\n",
    "print(f\"CLS输出形状: {cls_out.shape}\")    # (2, 768)\n",
    "print(f\"Patch输出形状: {patch_out.shape}\")  # (2, 196, 768)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64227bf",
   "metadata": {},
   "source": [
    "# 6. 实现Text Encoder (BERT-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    \"\"\"文本嵌入层\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embed = nn.Embedding(max_position, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        nn.init.normal_(self.token_embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.position_embed.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        token_embeds = self.token_embed(input_ids)\n",
    "        position_embeds = self.position_embed(position_ids)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    文本编码器 (BERT架构)\n",
    "    \n",
    "    输入: (B, seq_len) token IDs\n",
    "    输出: (B, seq_len, 768) 文本特征\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = TextEmbedding(vocab_size, embed_dim, max_position, dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        \n",
    "        # 创建注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "        else:\n",
    "            extended_mask = None\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, extended_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "print(\"测试Text Encoder...\")\n",
    "text_encoder = TextEncoder(depth=4)\n",
    "test_text = torch.randint(0, 30524, (2, 32))\n",
    "text_out = text_encoder(test_text)\n",
    "print(f\"Text输出形状: {text_out.shape}\")  # (2, 32, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a361c5",
   "metadata": {},
   "source": [
    "# 7. 实现Text Decoder (带Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    交叉注意力块 (用于解码器)\n",
    "    \n",
    "    包含:\n",
    "    1. Self-Attention (带因果掩码)\n",
    "    2. Cross-Attention (接收编码器输出)\n",
    "    3. MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states, self_attention_mask=None, cross_attention_mask=None):\n",
    "        # Self-Attention\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), attention_mask=self_attention_mask))\n",
    "        \n",
    "        # Cross-Attention\n",
    "        x = x + self.dropout(self.cross_attn(\n",
    "            self.norm2(x), \n",
    "            key=encoder_hidden_states, \n",
    "            value=encoder_hidden_states,\n",
    "            attention_mask=cross_attention_mask\n",
    "        ))\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp(self.norm3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    文本解码器 (用于生成任务)\n",
    "    \n",
    "    通过Cross-Attention接收图像特征\n",
    "    自回归生成文本\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = TextEmbedding(vocab_size, embed_dim, max_position, dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            CrossAttentionBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def _generate_causal_mask(self, seq_length, device):\n",
    "        \"\"\"生成因果掩码 (下三角矩阵)\"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, input_ids, encoder_hidden_states, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        x = self.embeddings(input_ids)\n",
    "        \n",
    "        # 因果掩码\n",
    "        causal_mask = self._generate_causal_mask(seq_length, input_ids.device)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "            causal_mask = causal_mask + extended_mask\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, encoder_hidden_states, causal_mask, None)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf93ce1",
   "metadata": {},
   "source": [
    "# 8. 完整BLIP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567078c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的BLIP模型\n",
    "    \n",
    "    支持:\n",
    "    - 图文检索 (Image-Text Retrieval)\n",
    "    - 图像描述生成\n",
    "    - 视觉问答 (VQA)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, img_size=224, patch_size=16, \n",
    "                 embed_dim=768, vision_depth=12, text_depth=12, num_heads=12, \n",
    "                 mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 视觉编码器\n",
    "        self.vision_encoder = VisionEncoder(\n",
    "            img_size, patch_size, 3, embed_dim, vision_depth, \n",
    "            num_heads, mlp_ratio, dropout\n",
    "        )\n",
    "        \n",
    "        # 文本编码器\n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size, embed_dim, text_depth, num_heads, \n",
    "            mlp_ratio, max_position, dropout\n",
    "        )\n",
    "        \n",
    "        # 文本解码器\n",
    "        self.text_decoder = TextDecoder(\n",
    "            vocab_size, embed_dim, text_depth, num_heads, \n",
    "            mlp_ratio, max_position, dropout\n",
    "        )\n",
    "        \n",
    "        # 投影层\n",
    "        self.vision_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.text_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # 图文匹配头\n",
    "        self.itm_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # 温度参数\n",
    "        self.temp = nn.Parameter(torch.ones(1) * 0.07)\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        \"\"\"编码图像\"\"\"\n",
    "        cls_output, _ = self.vision_encoder(image)\n",
    "        image_embed = self.vision_proj(cls_output)\n",
    "        image_embed = F.normalize(image_embed, dim=-1)\n",
    "        return image_embed\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask=None):\n",
    "        \"\"\"编码文本\"\"\"\n",
    "        text_output = self.text_encoder(input_ids, attention_mask)\n",
    "        text_embed = self.text_proj(text_output[:, 0])  # CLS token\n",
    "        text_embed = F.normalize(text_embed, dim=-1)\n",
    "        return text_embed\n",
    "    \n",
    "    def compute_similarity(self, image, input_ids, attention_mask=None):\n",
    "        \"\"\"计算图文相似度\"\"\"\n",
    "        image_embed = self.encode_image(image)\n",
    "        text_embed = self.encode_text(input_ids, attention_mask)\n",
    "        similarity = image_embed @ text_embed.T / self.temp.exp()\n",
    "        return similarity\n",
    "    \n",
    "    def generate_caption(self, image, max_length=50, bos_token_id=101, eos_token_id=102):\n",
    "        \"\"\"生成图像描述\"\"\"\n",
    "        _, patch_output = self.vision_encoder(image)\n",
    "        \n",
    "        batch_size = image.size(0)\n",
    "        generated = torch.full((batch_size, 1), bos_token_id, dtype=torch.long, device=image.device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = self.text_decoder(generated, patch_output)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            \n",
    "            if (next_token == eos_token_id).all():\n",
    "                break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd0156",
   "metadata": {},
   "source": [
    "# 9.测试完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3068fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型 (使用较小的配置快速测试)\n",
    "print(\"创建BLIP模型...\")\n",
    "model = BLIPModel(\n",
    "    vocab_size=30524,\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    vision_depth=6,   # 减少层数\n",
    "    text_depth=6,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0\n",
    ")\n",
    "\n",
    "# 统计参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n模型参数统计:\")\n",
    "print(f\"  总参数: {total_params:,}\")\n",
    "print(f\"  可训练参数: {trainable_params:,}\")\n",
    "\n",
    "# 测试前向传播\n",
    "print(\"测试前向传播...\")\n",
    "\n",
    "batch_size = 2\n",
    "image = torch.randn(batch_size, 3, 224, 224)\n",
    "input_ids = torch.randint(0, 30524, (batch_size, 32))\n",
    "attention_mask = torch.ones(batch_size, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 图像编码\n",
    "    image_embed = model.encode_image(image)\n",
    "    print(f\"图像特征: {image_embed.shape}\")\n",
    "    \n",
    "    # 文本编码\n",
    "    text_embed = model.encode_text(input_ids, attention_mask)\n",
    "    print(f\"文本特征: {text_embed.shape}\")\n",
    "    \n",
    "    # 相似度计算\n",
    "    similarity = model.compute_similarity(image, input_ids, attention_mask)\n",
    "    print(f\"相似度矩阵: {similarity.shape}\")\n",
    "    \n",
    "    # 图像描述生成\n",
    "    caption = model.generate_caption(image, max_length=10)\n",
    "    print(f\"生成的描述token: {caption.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
