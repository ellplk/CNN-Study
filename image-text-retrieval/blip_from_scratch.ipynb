{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¨ ä»é›¶å®ç°BLIPæ¨¡å‹\n",
    "\n",
    "æœ¬Notebookä»é›¶å®ç°BLIPçš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "- **Vision Encoder**: ViTæ¶æ„\n",
    "- **Text Encoder**: BERTæ¶æ„\n",
    "- **Text Decoder**: å¸¦Cross-Attentionçš„è§£ç å™¨\n",
    "- **å¤šæ¨¡æ€èåˆ**: å›¾æ–‡åŒ¹é…ä¸ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å®ç°Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    å°†å›¾åƒåˆ†å‰²æˆpatchå¹¶è¿›è¡ŒåµŒå…¥\n",
    "    \n",
    "    è¾“å…¥: (B, 3, 224, 224)\n",
    "    è¾“å‡º: (B, 196, 768)  # 14x14 = 196 patches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # 14x14 = 196\n",
    "        \n",
    "        # ä½¿ç”¨å·ç§¯å°†patchè½¬æ¢ä¸ºå‘é‡\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, 224, 224)\n",
    "        x = self.proj(x)  # (B, 768, 14, 14)\n",
    "        x = x.flatten(2)  # (B, 768, 196)\n",
    "        x = x.transpose(1, 2)  # (B, 196, 768)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯•\n",
    "patch_embed = PatchEmbedding()\n",
    "test_img = torch.randn(2, 3, 224, 224)\n",
    "output = patch_embed(test_img)\n",
    "print(f\"Patch Embeddingè¾“å‡ºå½¢çŠ¶: {output.shape}\")  # (2, 196, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "    \n",
    "    æ”¯æŒä¸‰ç§æ¨¡å¼:\n",
    "    1. Self-Attention: Q = K = V\n",
    "    2. Cross-Attention: Qæ¥è‡ªä¸€ä¸ªåºåˆ—, K,Væ¥è‡ªå¦ä¸€ä¸ªåºåˆ—\n",
    "    3. Masked Self-Attention: ç”¨äºè§£ç å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # 64\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Q, K, VæŠ•å½±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key=None, value=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (B, N, C) æŸ¥è¯¢åºåˆ—\n",
    "            key: (B, M, C) é”®åºåˆ— (Noneåˆ™ä½¿ç”¨query)\n",
    "            value: (B, M, C) å€¼åºåˆ— (Noneåˆ™ä½¿ç”¨query)\n",
    "            attention_mask: (B, 1, N, M) æ³¨æ„åŠ›æ©ç \n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "        \n",
    "        B, N, C = query.shape\n",
    "        \n",
    "        # çº¿æ€§æŠ•å½±å¹¶é‡å¡‘ä¸ºå¤šå¤´å½¢å¼\n",
    "        q = self.q_proj(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).reshape(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).reshape(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, M)\n",
    "        \n",
    "        # åº”ç”¨æ©ç \n",
    "        if attention_mask is not None:\n",
    "            attn = attn + attention_mask\n",
    "        \n",
    "        # Softmaxå½’ä¸€åŒ–\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # åŠ æƒæ±‚å’Œ\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.out_proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# æµ‹è¯•\n",
    "attn = MultiHeadAttention()\n",
    "test_input = torch.randn(2, 196, 768)\n",
    "output = attn(test_input)\n",
    "print(f\"Multi-Head Attentionè¾“å‡ºå½¢çŠ¶: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å®ç°Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"å‰é¦ˆç¥ç»ç½‘ç»œ\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformerç¼–ç å™¨å—\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Pre-Normæ¶æ„\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), attention_mask=attention_mask))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å®ç°Vision Encoder (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    è§†è§‰ç¼–ç å™¨ (ViTæ¶æ„)\n",
    "    \n",
    "    è¾“å…¥: (B, 3, 224, 224)\n",
    "    è¾“å‡º: \n",
    "        - cls_output: (B, 768) å…¨å±€ç‰¹å¾\n",
    "        - patch_output: (B, 196, 768) å±€éƒ¨ç‰¹å¾\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # PatchåµŒå…¥\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # CLS tokenå’Œä½ç½®ç¼–ç \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformerå—\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # PatchåµŒå…¥\n",
    "        x = self.patch_embed(x)  # (B, 196, 768)\n",
    "        \n",
    "        # æ·»åŠ CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 197, 768)\n",
    "        \n",
    "        # æ·»åŠ ä½ç½®ç¼–ç \n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformerç¼–ç \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # åˆ†ç¦»CLS tokenå’Œpatch tokens\n",
    "        cls_output = x[:, 0]      # å…¨å±€ç‰¹å¾\n",
    "        patch_output = x[:, 1:]   # å±€éƒ¨ç‰¹å¾\n",
    "        \n",
    "        return cls_output, patch_output\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"æµ‹è¯•Vision Encoder...\")\n",
    "vision_encoder = VisionEncoder(depth=4)  # ä½¿ç”¨è¾ƒå°‘å±‚æ•°å¿«é€Ÿæµ‹è¯•\n",
    "test_img = torch.randn(2, 3, 224, 224)\n",
    "cls_out, patch_out = vision_encoder(test_img)\n",
    "print(f\"CLSè¾“å‡ºå½¢çŠ¶: {cls_out.shape}\")    # (2, 768)\n",
    "print(f\"Patchè¾“å‡ºå½¢çŠ¶: {patch_out.shape}\")  # (2, 196, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å®ç°Text Encoder (BERT-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    \"\"\"æ–‡æœ¬åµŒå…¥å±‚\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embed = nn.Embedding(max_position, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        nn.init.normal_(self.token_embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.position_embed.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        token_embeds = self.token_embed(input_ids)\n",
    "        position_embeds = self.position_embed(position_ids)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬ç¼–ç å™¨ (BERTæ¶æ„)\n",
    "    \n",
    "    è¾“å…¥: (B, seq_len) token IDs\n",
    "    è¾“å‡º: (B, seq_len, 768) æ–‡æœ¬ç‰¹å¾\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = TextEmbedding(vocab_size, embed_dim, max_position, dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        \n",
    "        # åˆ›å»ºæ³¨æ„åŠ›æ©ç \n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "        else:\n",
    "            extended_mask = None\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, extended_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"æµ‹è¯•Text Encoder...\")\n",
    "text_encoder = TextEncoder(depth=4)\n",
    "test_text = torch.randint(0, 30524, (2, 32))\n",
    "text_out = text_encoder(test_text)\n",
    "print(f\"Textè¾“å‡ºå½¢çŠ¶: {text_out.shape}\")  # (2, 32, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å®ç°Text Decoder (å¸¦Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    äº¤å‰æ³¨æ„åŠ›å— (ç”¨äºè§£ç å™¨)\n",
    "    \n",
    "    åŒ…å«:\n",
    "    1. Self-Attention (å¸¦å› æœæ©ç )\n",
    "    2. Cross-Attention (æ¥æ”¶ç¼–ç å™¨è¾“å‡º)\n",
    "    3. MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states, self_attention_mask=None, cross_attention_mask=None):\n",
    "        # Self-Attention\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), attention_mask=self_attention_mask))\n",
    "        \n",
    "        # Cross-Attention\n",
    "        x = x + self.dropout(self.cross_attn(\n",
    "            self.norm2(x), \n",
    "            key=encoder_hidden_states, \n",
    "            value=encoder_hidden_states,\n",
    "            attention_mask=cross_attention_mask\n",
    "        ))\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp(self.norm3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬è§£ç å™¨ (ç”¨äºç”Ÿæˆä»»åŠ¡)\n",
    "    \n",
    "    é€šè¿‡Cross-Attentionæ¥æ”¶å›¾åƒç‰¹å¾\n",
    "    è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = TextEmbedding(vocab_size, embed_dim, max_position, dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            CrossAttentionBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def _generate_causal_mask(self, seq_length, device):\n",
    "        \"\"\"ç”Ÿæˆå› æœæ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ)\"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, input_ids, encoder_hidden_states, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        x = self.embeddings(input_ids)\n",
    "        \n",
    "        # å› æœæ©ç \n",
    "        causal_mask = self._generate_causal_mask(seq_length, input_ids.device)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "            causal_mask = causal_mask + extended_mask\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, encoder_hidden_states, causal_mask, None)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å®Œæ•´BLIPæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„BLIPæ¨¡å‹\n",
    "    \n",
    "    æ”¯æŒ:\n",
    "    - å›¾æ–‡æ£€ç´¢ (Image-Text Retrieval)\n",
    "    - å›¾åƒæè¿°ç”Ÿæˆ\n",
    "    - è§†è§‰é—®ç­” (VQA)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30524, img_size=224, patch_size=16, \n",
    "                 embed_dim=768, vision_depth=12, text_depth=12, num_heads=12, \n",
    "                 mlp_ratio=4.0, max_position=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è§†è§‰ç¼–ç å™¨\n",
    "        self.vision_encoder = VisionEncoder(\n",
    "            img_size, patch_size, 3, embed_dim, vision_depth, \n",
    "            num_heads, mlp_ratio, dropout\n",
    "        )\n",
    "        \n",
    "        # æ–‡æœ¬ç¼–ç å™¨\n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size, embed_dim, text_depth, num_heads, \n",
    "            mlp_ratio, max_position, dropout\n",
    "        )\n",
    "        \n",
    "        # æ–‡æœ¬è§£ç å™¨\n",
    "        self.text_decoder = TextDecoder(\n",
    "            vocab_size, embed_dim, text_depth, num_heads, \n",
    "            mlp_ratio, max_position, dropout\n",
    "        )\n",
    "        \n",
    "        # æŠ•å½±å±‚\n",
    "        self.vision_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.text_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # å›¾æ–‡åŒ¹é…å¤´\n",
    "        self.itm_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # æ¸©åº¦å‚æ•°\n",
    "        self.temp = nn.Parameter(torch.ones(1) * 0.07)\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        \"\"\"ç¼–ç å›¾åƒ\"\"\"\n",
    "        cls_output, _ = self.vision_encoder(image)\n",
    "        image_embed = self.vision_proj(cls_output)\n",
    "        image_embed = F.normalize(image_embed, dim=-1)\n",
    "        return image_embed\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask=None):\n",
    "        \"\"\"ç¼–ç æ–‡æœ¬\"\"\"\n",
    "        text_output = self.text_encoder(input_ids, attention_mask)\n",
    "        text_embed = self.text_proj(text_output[:, 0])  # CLS token\n",
    "        text_embed = F.normalize(text_embed, dim=-1)\n",
    "        return text_embed\n",
    "    \n",
    "    def compute_similarity(self, image, input_ids, attention_mask=None):\n",
    "        \"\"\"è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦\"\"\"\n",
    "        image_embed = self.encode_image(image)\n",
    "        text_embed = self.encode_text(input_ids, attention_mask)\n",
    "        similarity = image_embed @ text_embed.T / self.temp.exp()\n",
    "        return similarity\n",
    "    \n",
    "    def generate_caption(self, image, max_length=50, bos_token_id=101, eos_token_id=102):\n",
    "        \"\"\"ç”Ÿæˆå›¾åƒæè¿°\"\"\"\n",
    "        _, patch_output = self.vision_encoder(image)\n",
    "        \n",
    "        batch_size = image.size(0)\n",
    "        generated = torch.full((batch_size, 1), bos_token_id, dtype=torch.long, device=image.device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = self.text_decoder(generated, patch_output)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            \n",
    "            if (next_token == eos_token_id).all():\n",
    "                break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•å®Œæ•´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹ (ä½¿ç”¨è¾ƒå°çš„é…ç½®å¿«é€Ÿæµ‹è¯•)\n",
    "print(\"åˆ›å»ºBLIPæ¨¡å‹...\")\n",
    "model = BLIPModel(\n",
    "    vocab_size=30524,\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    vision_depth=6,   # å‡å°‘å±‚æ•°\n",
    "    text_depth=6,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0\n",
    ")\n",
    "\n",
    "# ç»Ÿè®¡å‚æ•°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\næ¨¡å‹å‚æ•°ç»Ÿè®¡:\")\n",
    "print(f\"  æ€»å‚æ•°: {total_params:,}\")\n",
    "print(f\"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "print(\"æµ‹è¯•å‰å‘ä¼ æ’­...\")\n",
    "\n",
    "batch_size = 2\n",
    "image = torch.randn(batch_size, 3, 224, 224)\n",
    "input_ids = torch.randint(0, 30524, (batch_size, 32))\n",
    "attention_mask = torch.ones(batch_size, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # å›¾åƒç¼–ç \n",
    "    image_embed = model.encode_image(image)\n",
    "    print(f\"å›¾åƒç‰¹å¾: {image_embed.shape}\")\n",
    "    \n",
    "    # æ–‡æœ¬ç¼–ç \n",
    "    text_embed = model.encode_text(input_ids, attention_mask)\n",
    "    print(f\"æ–‡æœ¬ç‰¹å¾: {text_embed.shape}\")\n",
    "    \n",
    "    # ç›¸ä¼¼åº¦è®¡ç®—\n",
    "    similarity = model.compute_similarity(image, input_ids, attention_mask)\n",
    "    print(f\"ç›¸ä¼¼åº¦çŸ©é˜µ: {similarity.shape}\")\n",
    "    \n",
    "    # å›¾åƒæè¿°ç”Ÿæˆ\n",
    "    caption = model.generate_caption(image, max_length=10)\n",
    "    print(f\"ç”Ÿæˆçš„æè¿°token: {caption.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ¨¡å‹æ¶æ„å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_structure():\n",
    "    \"\"\"æ‰“å°æ¨¡å‹ç»“æ„\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLIPæ¨¡å‹æ¶æ„\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚                     Vision Encoder (ViT)                       â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"â”‚  è¾“å…¥: (B, 3, 224, 224)                                        â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  Patch Embedding: 16Ã—16 patches â†’ 196 tokens                   â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  + CLS Token + Position Embedding                              â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  Transformer Blocks Ã— 12                                       â”‚\")\n",
    "    print(\"â”‚    - Multi-Head Self-Attention (12 heads)                      â”‚\")\n",
    "    print(\"â”‚    - MLP (768 â†’ 3072 â†’ 768)                                    â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  è¾“å‡º: CLS (768) + Patches (196, 768)                          â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    \n",
    "    print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚                     Text Encoder (BERT)                        â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"â”‚  è¾“å…¥: Token IDs (B, seq_len)                                  â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  Token Embedding + Position Embedding                          â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  Transformer Blocks Ã— 12                                       â”‚\")\n",
    "    print(\"â”‚    - Multi-Head Self-Attention (åŒå‘)                          â”‚\")\n",
    "    print(\"â”‚    - MLP                                                       â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  è¾“å‡º: (B, seq_len, 768)                                       â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    \n",
    "    print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚                     Text Decoder                               â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"â”‚  è¾“å…¥: Token IDs + Encoder Hidden States                       â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  Cross-Attention Blocks Ã— 12                                   â”‚\")\n",
    "    print(\"â”‚    - Self-Attention (å› æœæ©ç )                                 â”‚\")\n",
    "    print(\"â”‚    - Cross-Attention (æ¥æ”¶å›¾åƒç‰¹å¾)                            â”‚\")\n",
    "    print(\"â”‚    - MLP                                                       â”‚\")\n",
    "    print(\"â”‚    â†“                                                           â”‚\")\n",
    "    print(\"â”‚  è¾“å‡º: Logits (B, seq_len, vocab_size)                         â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "print_model_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. æ€»ç»“\n",
    "\n",
    "### ä»é›¶å®ç°çš„ç»„ä»¶\n",
    "\n",
    "| ç»„ä»¶ | å®ç° | å‚æ•°é‡ |\n",
    "|------|------|--------|\n",
    "| Patch Embedding | Conv2dæŠ•å½± | ~0.6M |\n",
    "| Multi-Head Attention | Q/K/VæŠ•å½± + Softmax | ~2.4M |\n",
    "| Vision Encoder | ViTæ¶æ„ | ~86M |\n",
    "| Text Encoder | BERTæ¶æ„ | ~102M |\n",
    "| Text Decoder | Cross-Attention | ~102M |\n",
    "| æ€»è®¡ | - | ~290M |\n",
    "\n",
    "### æ ¸å¿ƒæŠ€æœ¯ç‚¹\n",
    "\n",
    "1. **ViTæ¶æ„**: å°†å›¾åƒåˆ†æˆpatchï¼Œç”¨Transformerå¤„ç†\n",
    "2. **BERTæ¶æ„**: åŒå‘è‡ªæ³¨æ„åŠ›ï¼Œç”¨äºæ–‡æœ¬ç†è§£\n",
    "3. **Cross-Attention**: è§£ç å™¨é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¥æ”¶å›¾åƒä¿¡æ¯\n",
    "4. **å¯¹æ¯”å­¦ä¹ **: å›¾æ–‡ç‰¹å¾å¯¹é½åˆ°åŒä¸€ç©ºé—´\n",
    "\n",
    "### ä¸é¢„è®­ç»ƒæ¨¡å‹çš„åŒºåˆ«\n",
    "\n",
    "| æ–¹é¢ | ä»é›¶å®ç° | é¢„è®­ç»ƒæ¨¡å‹ |\n",
    "|------|----------|------------|\n",
    "| æƒé‡ | éšæœºåˆå§‹åŒ– | å¤§è§„æ¨¡é¢„è®­ç»ƒ |\n",
    "| æ€§èƒ½ | éœ€è¦è®­ç»ƒ | ç›´æ¥å¯ç”¨ |\n",
    "| ç†è§£ | å®Œå…¨é€æ˜ | é»‘ç›’ä½¿ç”¨ |\n",
    "| å®šåˆ¶ | å®Œå…¨å¯æ§ | å—é™ä¿®æ”¹ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
