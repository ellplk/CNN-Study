import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import gc
import os

# RTX 4060 8GB ä¼˜åŒ–ç‰ˆViT
class ViTCIFAR10_RTX4060(nn.Module):
    def __init__(self, num_classes=10, pretrained=True):
        super(ViTCIFAR10_RTX4060, self).__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„ViTæ¨¡å‹
        if pretrained:
            weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1
        else:
            weights = None
        self.vit = torchvision.models.vit_b_16(weights=weights)
        
        # è·å–é¢„è®­ç»ƒæ¨¡å‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
        self.vit_expected_input_size = self.vit.image_size
        
        # ä¿®æ”¹åˆ†ç±»å¤´
        self.vit.heads = nn.Linear(self.vit.heads.head.in_features, num_classes)
        
    def forward(self, x):
        # åŠ¨æ€è°ƒæ•´è¾“å…¥å°ºå¯¸åˆ°ViTæœŸæœ›çš„224x224
        current_h, current_w = x.shape[-2:]
        expected_size = self.vit_expected_input_size

        if current_h != expected_size or current_w != expected_size:
            x = torch.nn.functional.interpolate(x, 
                                              size=(expected_size, expected_size),
                                              mode='bilinear', align_corners=False)
        
        result = self.vit(x)
        return result

def check_gpu_setup():
    """æ£€æŸ¥GPUé…ç½®å¹¶é€‰æ‹©RTX 4060"""
    print("=== RTX 4060 GPUè®¾ç½®æ£€æŸ¥ ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # æŸ¥æ‰¾RTX 4060
        rtx4060_device = None
        for i in range(torch.cuda.device_count()):
            gpu_props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {gpu_props.name}")
            print(f"  æ˜¾å­˜: {gpu_props.total_memory / 1024**3:.2f}GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
            
            # æŸ¥æ‰¾RTX 4060
            if "4060" in gpu_props.name or "RTX 4060" in gpu_props.name:
                rtx4060_device = i
                print(f"  âœ… æ‰¾åˆ°RTX 4060!")
        
        if rtx4060_device is not None:
            # ä½¿ç”¨RTX 4060
            torch.cuda.set_device(rtx4060_device)
            print(f"è®¾ç½®ä½¿ç”¨GPU {rtx4060_device}: {torch.cuda.get_device_properties(rtx4060_device).name}")
            device = torch.device(f'cuda:{rtx4060_device}')
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°RTX 4060ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªGPUï¼ˆé€šå¸¸æ˜¯ç‹¬æ˜¾ï¼‰
            rtx4060_device = torch.cuda.device_count() - 1
            torch.cuda.set_device(rtx4060_device)
            print(f"æœªæ‰¾åˆ°RTX 4060ï¼Œä½¿ç”¨GPU {rtx4060_device}: {torch.cuda.get_device_properties(rtx4060_device).name}")
            device = torch.device(f'cuda:{rtx4060_device}')
        
        print(f"å½“å‰ä½¿ç”¨GPU: {torch.cuda.current_device()}")
        
        # æ¸…ç©ºæ˜¾å­˜ç¼“å­˜
        torch.cuda.empty_cache()
        print("å·²æ¸…ç©ºGPUç¼“å­˜")
        
        return device
    else:
        print("CUDAä¸å¯ç”¨ï¼Œç¨‹åºå°†é€€å‡º")
        exit(1)

def main():
    # æ£€æŸ¥GPUè®¾ç½®å¹¶è·å–æ­£ç¡®çš„è®¾å¤‡
    device = check_gpu_setup()
    print("=" * 35)
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')
    
    # RTX 4060 8GB ä¼˜åŒ–å‚æ•°
    batch_size = 64         # é’ˆå¯¹8GBæ˜¾å­˜ä¼˜åŒ–çš„batch size
    learning_rate = 3e-4    # é€‚åˆViTçš„å­¦ä¹ ç‡
    num_epochs = 25
    input_size = 160        # åˆå§‹è¾“å…¥å°ºå¯¸ï¼Œä¼šè‡ªåŠ¨è°ƒæ•´åˆ°224
    
    # æ•°æ®é¢„å¤„ç† - ImageNeté¢„è®­ç»ƒæƒé‡çš„æ ‡å‡†åŒ–å‚æ•°
    transform_train = transforms.Compose([
        transforms.Resize((input_size, input_size)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize((input_size, input_size)),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    ])
    
    # åŠ è½½CIFAR-10æ•°æ®é›†
    print("åŠ è½½CIFAR-10æ•°æ®é›†...")
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train)
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test)
    
    # ä¼˜åŒ–çš„DataLoaderè®¾ç½®
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=6,          # å……åˆ†åˆ©ç”¨CPU
        pin_memory=True,        # åŠ é€ŸGPUä¼ è¾“
        persistent_workers=True, # å‡å°‘workeré‡å¯å¼€é”€
        prefetch_factor=3       # é¢„å–æ•°æ®
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size,  # æµ‹è¯•æ—¶ä½¿ç”¨ç›¸åŒbatch size
        shuffle=False, 
        num_workers=6,
        pin_memory=True,
        persistent_workers=True
    )
    
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, æµ‹è¯•æ ·æœ¬: {len(test_dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºViTæ¨¡å‹...")
    model = ViTCIFAR10_RTX4060(num_classes=10, pretrained=True).to(device)
    
    # æ¨¡å‹å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=learning_rate, 
        weight_decay=0.05,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - Warmup + Cosine
    warmup_epochs = 3
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs-warmup_epochs)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.amp.GradScaler('cuda')
    
    # è®­ç»ƒçŠ¶æ€
    best_accuracy = 0
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    print(f"\nå¼€å§‹è®­ç»ƒ - {num_epochs} epochs")
    print("=" * 60)
    
    for epoch in range(num_epochs):
        # === è®­ç»ƒé˜¶æ®µ ===
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        # Warmupå­¦ä¹ ç‡
        if epoch < warmup_epochs:
            lr_scale = (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = learning_rate * lr_scale
        
        for i, (images, labels) in enumerate(train_loader):
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda'):
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            
            # æ˜¾å­˜ç®¡ç†
            if (i + 1) % 100 == 0:
                torch.cuda.empty_cache()
                
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 150 == 0:
                current_acc = 100 * correct_train / total_train
                allocated = torch.cuda.memory_allocated(device) / 1024**3
                reserved = torch.cuda.memory_reserved(device) / 1024**3
                print(f'Epoch [{epoch+1:2d}/{num_epochs}] '
                      f'Step [{i+1:4d}/{len(train_loader)}] '
                      f'Loss: {loss.item():.4f} '
                      f'Acc: {current_acc:.2f}% '
                      f'GPU: {allocated:.2f}/{reserved:.2f}GB '
                      f'Device: {device}')
        
        # æ›´æ–°å­¦ä¹ ç‡
        if epoch >= warmup_epochs:
            scheduler.step()
        
        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        train_accuracy = 100 * correct_train / total_train
        avg_train_loss = running_loss / len(train_loader)
        
        # === æµ‹è¯•é˜¶æ®µ ===
        model.eval()
        correct_test = 0
        total_test = 0
        test_loss = 0.0
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                
                with torch.amp.autocast('cuda'):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                
                test_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total_test += labels.size(0)
                correct_test += (predicted == labels).sum().item()
        
        test_accuracy = 100 * correct_test / total_test
        avg_test_loss = test_loss / len(test_loader)
        
        # è®°å½•å†å²
        train_losses.append(avg_train_loss)
        train_accuracies.append(train_accuracy)
        test_accuracies.append(test_accuracy)
        
        # æ˜¾ç¤ºç»“æœ
        current_lr = optimizer.param_groups[0]['lr']
        allocated = torch.cuda.memory_allocated(device) / 1024**3
        reserved = torch.cuda.memory_reserved(device) / 1024**3
        
        print(f'\nEpoch [{epoch+1:2d}/{num_epochs}] å®Œæˆ')
        print(f'è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.2f}%')
        print(f'æµ‹è¯• - Loss: {avg_test_loss:.4f}, Acc: {test_accuracy:.2f}%')
        print(f'å­¦ä¹ ç‡: {current_lr:.6f}')
        print(f'æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {reserved:.2f}GB (GPU: {device})')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_accuracy': best_accuracy,
                'train_losses': train_losses,
                'train_accuracies': train_accuracies,
                'test_accuracies': test_accuracies
            }, 'vit_rtx4060_best.pth')
            print(f'ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {best_accuracy:.2f}%')
        
        print('-' * 60)
        
        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
    
    # è®­ç»ƒå®Œæˆ
    print(f'\nğŸ‰ è®­ç»ƒå®Œæˆ!')
    print(f'ğŸ“Š æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_accuracy:.2f}%')
    print(f'ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: vit_rtx4060_best.pth')
    
    # æœ€ç»ˆæ˜¾å­˜æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

if __name__ == '__main__':
    # ç§»é™¤CUDA_LAUNCH_BLOCKINGä»¥æé«˜æ€§èƒ½
    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # æ³¨é‡Šæ‰ä»¥æé«˜æ€§èƒ½
    
    # å¯ç”¨cuDNNä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    print("ğŸš€ RTX 4060 8GB ViT-CIFAR10 è®­ç»ƒç¨‹åº")
    print("=" * 50)
    
    main()